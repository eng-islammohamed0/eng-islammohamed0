{"cells":[{"source":"![mobydick](mobydick.jpg)","metadata":{},"id":"b1309988-b429-4fb0-8c4c-193582dbec93","cell_type":"markdown"},{"source":"In this workspace, you'll scrape the novel Moby Dick from the website [Project Gutenberg](https://www.gutenberg.org/) (which contains a large corpus of books) using the Python `requests` package. You'll extract words from this web data using `BeautifulSoup` before analyzing the distribution of words using the Natural Language ToolKit (`nltk`) and `Counter`.\n\nThe Data Science pipeline you'll build in this workspace can be used to visualize the word frequency distributions of any novel you can find on Project Gutenberg.","metadata":{},"id":"611e416c-70e7-478a-a3c8-e54f3fdb4a7f","cell_type":"markdown"},{"source":"# Import and download packages\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom collections import Counter\nnltk.download('stopwords')\n\n# Start coding here... ","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"executionTime":null,"lastSuccessfullyExecutedCode":null,"executionCancelledAt":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"outputsMetadata":{"0":{"height":59,"type":"stream"}}},"id":"15b5f52f-fd9b-4f0e-9fcc-f7733022c7c0","cell_type":"code","execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package stopwords to /home/repl/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n"},{"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{},"execution_count":1}]},{"source":"# Import and download packages\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom collections import Counter\nnltk.download('stopwords')\n\n# Get the Moby Dick HTML  \nr = requests.get('https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm')\n\n# Set the correct text encoding of the HTML page\nr.encoding = 'utf-8'\n\n# Extract the HTML from the request object\nhtml = r.text\n\n# Print the first 2000 characters in html\nprint(html[0:2000])\n\n# Create a BeautifulSoup object from the HTML\nhtml_soup = BeautifulSoup(html, \"html.parser\")\n\n# Get the text out of the soup\nmoby_text = html_soup.get_text()\n\n# Create a tokenizer\ntokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n\n# Tokenize the text\ntokens = tokenizer.tokenize(moby_text)\n\n# Create a list called words containing all tokens transformed to lowercase\nwords = [token.lower() for token in tokens]\n\n# Print out the first eight words\nwords[:8]\n\n# Get the English stop words from nltk\nstop_words = nltk.corpus.stopwords.words('english')\n\n# Print out the first eight stop words\nstop_words[:8]\n\n# Create a list words_ns containing all words that are in words but not in stop_words\nwords_no_stop = [word for word in words if word not in stop_words]\n\n# Print the first five words_no_stop to check that stop words are gone\nwords_no_stop[:5]\n\n# Initialize a Counter object from our processed list of words\ncount = Counter(words_no_stop)\n\n# Store ten most common words and their counts as top_ten\ntop_ten = count.most_common(10)\n\n# Print the top ten words and their counts\nprint(top_ten)","metadata":{},"cell_type":"code","id":"6ca456b2-59a9-44a1-bc9b-3d56b5ead754","outputs":[],"execution_count":null}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}